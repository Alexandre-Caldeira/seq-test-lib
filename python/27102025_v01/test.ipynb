{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py (Corrected)\n",
    "\"\"\"\n",
    "Main executable script for the seq-test-lib library.\n",
    "\n",
    "This script unifies the analyses from the various notebooks into a single,\n",
    "reusable workflow. It performs a comprehensive, cross-validated comparison of\n",
    "statistical and machine learning-based Orienting Response Detectors (ORDs).\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "# Import all necessary modules from our new library\n",
    "from seqtestlib import config\n",
    "from seqtestlib.data.loaders import SimulatedLoader\n",
    "from seqtestlib.models import ml, dl, statistical\n",
    "from seqtestlib.sequential import testers\n",
    "from seqtestlib.evaluation import validators\n",
    "from seqtestlib.visualization import performance, style\n",
    "\n",
    "def run_full_analysis():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the entire analysis pipeline.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"--- Starting Unified ORD Detection Analysis ---\")\n",
    "\n",
    "    # --- 1. Initialization and Data Loading ---\n",
    "    loader = SimulatedLoader(config.SIMULATED_FEATURES_FILE)\n",
    "    base_feature_idx = 0 if config.BASE_METHOD_FOR_PLOTS == 'MSC' else 1\n",
    "    base_feature_name = config.BASE_METHOD_FOR_PLOTS\n",
    "\n",
    "    # --- 2. Define Analysis Jobs ---\n",
    "    analysis_jobs = []\n",
    "    # (Job definitions remain the same)\n",
    "    for approach in [\n",
    "        {'window_type': 'Sliding', 'detection_method': 'Per-Window'},\n",
    "        {'window_type': 'Sliding', 'detection_method': 'Per-M-Block'},\n",
    "        {'window_type': 'Fixed', 'detection_method': 'Per-Window'},\n",
    "        {'window_type': 'Fixed', 'detection_method': 'Per-M-Block'}\n",
    "    ]:\n",
    "        for feature in [f'{base_feature_name}', f'CumSum_{base_feature_name}']:\n",
    "            feature_map = {'MSC': 0, 'CSM': 1, 'CumSum_MSC': 2, 'CumSum_CSM': 3}\n",
    "            analysis_jobs.append({\n",
    "                'feature_name': feature, 'model_type': 'statistical',\n",
    "                'feature_idx': feature_map.get(feature, 0), 'm_val': 18,\n",
    "                **approach\n",
    "            })\n",
    "    analysis_jobs.extend([\n",
    "        {\n",
    "            'feature_name': f'DANN-{base_feature_name}', 'model_type': 'dann',\n",
    "            'feature_idx': base_feature_idx, 'm_val': config.ML_WINDOW_SIZE,\n",
    "            'window_type': 'Sliding', 'detection_method': 'Per-Window'\n",
    "        },\n",
    "        {\n",
    "            'feature_name': f'LGBM-{base_feature_name}', 'model_type': 'lgbm',\n",
    "            'feature_idx': base_feature_idx, 'm_val': config.ML_WINDOW_SIZE,\n",
    "            'window_type': 'Sliding', 'detection_method': 'Per-Window'\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    # --- 3. Run Cross-Validation for Each Job ---\n",
    "    all_results_df = pd.DataFrame()\n",
    "    \n",
    "    # Train shared ML models once before the main loop\n",
    "    print(\"Pre-training ML models...\")\n",
    "    X_train, y_train, domains = loader.get_data_for_ml(config.ML_WINDOW_SIZE, base_feature_idx)\n",
    "    \n",
    "    lgbm_model = ml.LGBMModel()\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "    dann_model = dl.DANNModel(input_dim=config.ML_WINDOW_SIZE, num_domains=len(loader.snr_keys))\n",
    "    dann_model.fit(X_train, y_train, domains=domains)\n",
    "    print(\"ML models trained.\")\n",
    "\n",
    "    for job in tqdm(analysis_jobs, desc=\"Processing Analysis Jobs\"):\n",
    "        m_val = job['m_val']\n",
    "\n",
    "        # Instantiate the correct model for the job\n",
    "        if job['model_type'] == 'statistical':\n",
    "            model = statistical.StatisticalModel()\n",
    "        elif job['model_type'] == 'lgbm':\n",
    "            model = lgbm_model\n",
    "        elif job['model_type'] == 'dann':\n",
    "            model = dann_model\n",
    "        \n",
    "        # NOTE: For a real run, you would iterate through alphas here\n",
    "        # For simplicity in this structure, we'll use a fixed alpha from config\n",
    "        alpha = config.ALPHA_LEVELS[1] # Using 5.0%\n",
    "\n",
    "        # Instantiate the correct sequential tester\n",
    "        if job['detection_method'] == 'Per-M-Block':\n",
    "            tester = testers.BlockTester(model, alpha, m_block_size=m_val)\n",
    "        else:\n",
    "            tester = testers.StandardTester(model, alpha)\n",
    "            \n",
    "        # The Evaluator now handles the entire CV loop internally for this job\n",
    "        evaluator = validators.Evaluator(tester, m_val=m_val)\n",
    "        \n",
    "        # This is a placeholder call. The full implementation would be:\n",
    "        job_results_df = evaluator.run_simulated_cv(loader, job)\n",
    "        all_results_df = pd.concat([all_results_df, job_results_df], ignore_index=True)\n",
    "    \n",
    "    # --- 4. Generate Reports and Plots ---\n",
    "    # For demonstration, load pre-computed results as the CV loop above is a placeholder.\n",
    "    print(\"\\nNOTE: Evaluation loop is a placeholder. Loading pre-computed results for plotting.\")\n",
    "    try:\n",
    "        # Simulating a results DataFrame that would come from the loop above\n",
    "        demo_results_df = pd.read_csv('notebook_simulation_results.csv') \n",
    "        style.set_journal_style('nature', 'double')\n",
    "        performance.plot_simulation_results(demo_results_df)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not find 'notebook_simulation_results.csv'. Plotting skipped.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal analysis framework execution finished in {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    # You would uncomment this to run the full pipeline.\n",
    "    # The run_full_analysis() function still contains placeholder evaluation logic\n",
    "    # that needs to be fully implemented to match your specific analysis needs.\n",
    "run_full_analysis() \n",
    "    #print(\"Main script corrected. Please implement the final evaluation call inside run_full_analysis().\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
