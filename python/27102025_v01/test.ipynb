{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed7bbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Unified ORD Detection Analysis ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall CV Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training LGBM model on 35 samples...\n",
      "  Training DANN model on 35 samples (Device: cpu)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall CV Progress:   0%|          | 0/3 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StandardTester' object has no attribute '_get_noise_distribution'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 174\u001b[39m\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal analysis framework execution finished in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# if __name__ == '__main__':\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# Due to the complexity and long runtime, the evaluation logic inside the CV loop\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# is simplified. The structure is the key takeaway.\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# To run a full analysis, the placeholder evaluation section would need to be\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m# fully implemented to match the logic now in `evaluation/metrics.py`.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[43mrun_full_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Commented out to prevent accidental long run.\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# print(\"Main script structure is complete.\")\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# print(\"To run a full analysis, uncomment 'run_full_analysis()' and ensure the evaluation loop is fully implemented.\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 129\u001b[39m, in \u001b[36mrun_full_analysis\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    126\u001b[39m     tester = testers.StandardTester(model, alpha)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Set threshold based on training data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m noise_dist_data = \u001b[43mtester\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_noise_distribution\u001b[49m(data_source[dset_name], train_indices, loader.noise_bins, job)\n\u001b[32m    130\u001b[39m tester.set_threshold(noise_dist_data)\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Evaluate on test data across all SNRs\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'StandardTester' object has no attribute '_get_noise_distribution'"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\"\"\"\n",
    "Main executable script for the seq-test-lib library.\n",
    "\n",
    "This script unifies the analyses from the various notebooks into a single,\n",
    "reusable workflow. It performs a comprehensive, cross-validated comparison of\n",
    "statistical and machine learning-based Orienting Response Detectors (ORDs).\n",
    "\n",
    "Workflow:\n",
    "1.  Loads simulated data using the standardized loader.\n",
    "2.  Defines a series of analysis jobs for different methods (ORD, LGBM, DANN).\n",
    "3.  Iterates through cross-validation folds. In each fold:\n",
    "    a. Trains ML models and determines statistical thresholds on the training set.\n",
    "    b. Evaluates all methods on the test set across all SNR levels and alphas.\n",
    "4.  Consolidates and aggregates the results.\n",
    "5.  Generates and displays publication-quality plots and methodology reports.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "# Import all necessary modules from our new library\n",
    "from seqtestlib import config\n",
    "from seqtestlib.data.loaders import SimulatedLoader\n",
    "from seqtestlib.models import ml, dl, statistical\n",
    "from seqtestlib.sequential import testers, thresholds\n",
    "from seqtestlib.evaluation import metrics\n",
    "from seqtestlib.visualization import performance, style\n",
    "\n",
    "def run_full_analysis():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the entire analysis pipeline.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"--- Starting Unified ORD Detection Analysis ---\")\n",
    "\n",
    "    # --- 1. Initialization and Data Loading ---\n",
    "    loader = SimulatedLoader(config.SIMULATED_FEATURES_FILE)\n",
    "    num_trials_for_cv = min(config.MAX_SIM_TRIALS_FOR_ML_TRAINING, loader.num_trials)\n",
    "\n",
    "    # Define base feature for ML models from config\n",
    "    base_feature_idx = 0 if config.BASE_METHOD_FOR_PLOTS == 'MSC' else 1\n",
    "    base_feature_name = config.BASE_METHOD_FOR_PLOTS\n",
    "\n",
    "    # --- 2. Define Analysis Jobs ---\n",
    "    # These dictionaries define each detector and sequential test combination to run.\n",
    "    analysis_jobs = []\n",
    "    for approach in [\n",
    "        {'window_type': 'Sliding', 'detection_method': 'Per-Window'},\n",
    "        {'window_type': 'Sliding', 'detection_method': 'Per-M-Block'},\n",
    "        {'window_type': 'Fixed', 'detection_method': 'Per-Window'},\n",
    "        {'window_type': 'Fixed', 'detection_method': 'Per-M-Block'}\n",
    "    ]:\n",
    "        for feature in [f'{base_feature_name}', f'CumSum_{base_feature_name}']:\n",
    "            job = {\n",
    "                'feature_name': feature,\n",
    "                'model_type': 'statistical',\n",
    "                'feature_idx': config.SIMULATED_FEATURES_FILE.find(feature), # Placeholder, will be refined\n",
    "                'm_val': 18, # Using the M value from original ORD analysis\n",
    "                **approach\n",
    "            }\n",
    "            # Correctly map feature name to index from your original script\n",
    "            feature_map = {'MSC': 0, 'CSM': 1, 'CumSum_MSC': 2, 'CumSum_CSM': 3}\n",
    "            job['feature_idx'] = feature_map[feature]\n",
    "            analysis_jobs.append(job)\n",
    "\n",
    "    analysis_jobs.extend([\n",
    "        {\n",
    "            'feature_name': f'DANN-{base_feature_name}', 'model_type': 'dann',\n",
    "            'base_feature_name': base_feature_name, 'feature_idx': base_feature_idx,\n",
    "            'm_val': config.ML_WINDOW_SIZE, 'window_type': 'Sliding', 'detection_method': 'Per-Window'\n",
    "        },\n",
    "        {\n",
    "            'feature_name': f'LGBM-{base_feature_name}', 'model_type': 'lgbm',\n",
    "            'base_feature_name': base_feature_name, 'feature_idx': base_feature_idx,\n",
    "            'm_val': config.ML_WINDOW_SIZE, 'window_type': 'Sliding', 'detection_method': 'Per-Window'\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    # --- 3. Cross-Validation Loop ---\n",
    "    cv = ShuffleSplit(n_splits=config.N_SPLITS_SIM, test_size=config.TEST_SIZE_SIM, random_state=config.RANDOM_STATE)\n",
    "    all_results = []\n",
    "    \n",
    "    # Pre-load data for efficiency\n",
    "    data_source = {}\n",
    "    with h5py.File(loader.filepath, 'r') as f:\n",
    "        for job in analysis_jobs:\n",
    "            dset_name = loader._get_hdf5_dset_name(job['m_val'], job['window_type'])\n",
    "            if dset_name not in data_source:\n",
    "                data_source[dset_name] = {\n",
    "                    float(k.replace('snr_', '')): f[k][dset_name][:num_trials_for_cv]\n",
    "                    for k in loader.snr_keys\n",
    "                }\n",
    "\n",
    "    fold_pbar = tqdm(enumerate(cv.split(np.arange(num_trials_for_cv))), total=config.N_SPLITS_SIM, desc=\"Overall CV Progress\")\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in fold_pbar:\n",
    "        # Train ML models once per fold\n",
    "        lgbm_model = ml.LGBMModel()\n",
    "        X_train, y_train, _ = loader.get_data_for_ml(config.ML_WINDOW_SIZE, base_feature_idx)\n",
    "        lgbm_model.fit(X_train[train_indices], y_train[train_indices])\n",
    "\n",
    "        dann_model = dl.DANNModel(input_dim=config.ML_WINDOW_SIZE, num_domains=len(loader.snr_keys))\n",
    "        dann_model.fit(X_train[train_indices], y_train[train_indices], domains=_[train_indices])\n",
    "        \n",
    "        for job in analysis_jobs:\n",
    "            dset_name = loader._get_hdf5_dset_name(job['m_val'], job['window_type'])\n",
    "            \n",
    "            # Instantiate model\n",
    "            if job['model_type'] == 'statistical':\n",
    "                model = statistical.StatisticalModel()\n",
    "            elif job['model_type'] == 'lgbm':\n",
    "                model = lgbm_model\n",
    "            elif job['model_type'] == 'dann':\n",
    "                model = dann_model\n",
    "\n",
    "            for alpha in config.ALPHA_LEVELS:\n",
    "                # Instantiate tester for this alpha\n",
    "                if job['detection_method'] == 'Per-M-Block':\n",
    "                    tester = testers.BlockTester(model, alpha, m_block_size=job['m_val'])\n",
    "                else: # Per-Window\n",
    "                    tester = testers.StandardTester(model, alpha)\n",
    "                \n",
    "                # Set threshold based on training data\n",
    "                noise_dist_data = tester._get_noise_distribution(data_source[dset_name], train_indices, loader.noise_bins, job)\n",
    "                tester.set_threshold(noise_dist_data)\n",
    "\n",
    "                # Evaluate on test data across all SNRs\n",
    "                for snr, snr_data in data_source[dset_name].items():\n",
    "                    test_data_fold = snr_data[test_indices]\n",
    "                    \n",
    "                    # NOTE: This part is simplified for clarity. The validator would handle this logic.\n",
    "                    # A full implementation would need to feed appropriately shaped data to tester.test()\n",
    "                    # For now, we'll placeholder the evaluation part. This is where you would call your\n",
    "                    # refined evaluation logic that handles signal/noise bins separately.\n",
    "                    \n",
    "                    # Placeholder Evaluation:\n",
    "                    tpr, fpr, ttfd = 0.5, 0.05, 10 # Example fixed values\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        'SNR': snr, 'TPR': tpr, 'FPR': fpr, 'TTFD': ttfd,\n",
    "                        'Alpha (%)': alpha, 'Feature': job['feature_name'],\n",
    "                        'Window': job['window_type'], 'Detection': job.get('detection_method', 'ML')\n",
    "                    })\n",
    "    \n",
    "    # --- 4. Generate Reports and Plots ---\n",
    "    sim_results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # For demonstration, we'll load the results from your notebook's output\n",
    "    # to show that the plotting works. In a real run, you'd use sim_results_df.\n",
    "    print(\"\\nNOTE: Loading pre-computed results from notebook for plotting demonstration.\")\n",
    "    try:\n",
    "        demo_results_df = pd.read_csv('notebook_simulation_results.csv') # You would need to save your notebook's results to a CSV\n",
    "        style.set_journal_style('nature', 'double')\n",
    "        performance.plot_simulation_results(demo_results_df)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not find 'notebook_simulation_results.csv'. Plotting skipped.\")\n",
    "        print(\"To generate plots, run the unified notebook once and save 'sim_results_df' to CSV.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal analysis framework execution finished in {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    # Due to the complexity and long runtime, the evaluation logic inside the CV loop\n",
    "# is simplified. The structure is the key takeaway.\n",
    "# To run a full analysis, the placeholder evaluation section would need to be\n",
    "# fully implemented to match the logic now in `evaluation/metrics.py`.\n",
    "\n",
    "run_full_analysis() # Commented out to prevent accidental long run.\n",
    "# print(\"Main script structure is complete.\")\n",
    "# print(\"To run a full analysis, uncomment 'run_full_analysis()' and ensure the evaluation loop is fully implemented.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
